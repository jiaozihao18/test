在大模型兴起之后，推荐系统领域大体分化出了两种研究方向。一条路线是**把传统推荐模型做大做强**：继续沿用 Learning\-based Embedding Models（LEM）的基本范式，但把核心网络升级为 Transformer 或其变体，通过更强的序列建模能力和更好的 scaling 性能，提升效果。另一条路线则更激进，**直接把推荐问题重构为 token 级别的生成任务**，用序列到序列（seq2seq）的方式完成“检索 + 推荐”。

在第一条路线中，研究者主要聚焦于**用户行为序列建模和特征交互**。早期工作使用 RNN、CNN 等经典序列模型；后来开始设计各种复杂的特征交互模块；最近几年，Transformer 成为主流，用一个统一的序列来承载用户历史和异构特征，在大规模数据上表现出更好的可扩展性。比如HSTU（**Hierarchical Sequential Transduction Units**）是 Meta 在生成式推荐中的核心架构之一，它提出用 Transformer 栈和更长序列来建模用户行为和特征序列，支持超大参数规模并验证了 generative recommendation 的 scale law。美团在 HSTU 的基础上做了工业级落地，提出 **MTGR**：保留了传统推荐模型中的深度特征体系（包括交叉特征等），同时用 Transformer 统一多个序列，兼顾推荐效果和工况落地效率。

尽管这些方法在结构和训练方式上越来越“像大模型”，但本质上仍没有跳出传统推荐系统的核心假设：**大量离散特征必须通过 embedding table 映射成向量**。这意味着模型规模始终被 embedding 表主导，不仅内存和参数开销巨大，而且在冷启动、新物品、跨域泛化等问题上，先天受限。而第二条路线选择了**抛弃“大 embedding 表”这个包袱**。核心做法是：把原本的离散特征（尤其是 item）改写成**紧凑的输入 token**。TIGER提出 **Semantic ID** 作为 item 表征，用 token 序列替代传统 embedding ID。LC\-Rec 方案强调把语言模型语义和协同过滤信号 **对齐，**OneRec 等进一步引入了 **强化学习（RL）和偏好对齐目标**来做多目标优化甚至学习 platform 级策略（比如 CTR/CVR + 推荐质量综合优化）。

### llm4rec任务构建方案

在训练好语义ID模型后，基于qwen3 1.7b的小尺寸预训练模型作为基模，训练流程类似于模型增训微调，只不过新增了基于语义ID的token词表。为了促进语义id token与文本token融合，进行了三阶段训练，包括stage1新增词表冻结微调，stage2增量预训练以及stage3指令微调。

Stage1阶段，扩展模型embedding，只训练新增的embedding参数，采用较大的学习率（1e\-3）跑固定的步数。在Stage2开放所有权重，使用较低的学习率（2e\-5）跑3个epoch。预训练任务设计包括**语义锚定任务**：以语义ID（商品ID）为核心，生成包含电影标题、类型和剧情的完整自然语言描述，用于把离散ID锚定到可理解的语义空间；以及**属性映射子任务**：将语义ID分别映射到**标题、类型、剧情**等单一属性（sid→title / category / plot），强化模型对ID与关键属性之间对应关系的记忆。此外通过通过随机文本模板生成多样化表述，提升模型对同一语义在不同自然语言表达下的鲁棒性。举例如下：

```python
{"text": "商品<|sid_start|><|sid_112|><|sid_267|><|sid_645|><|sid_768|><|sid_end|>是一部题材和风格涵盖喜剧、音乐与表演、其他等类型的电影，电影名称是《Hee Haw：经典回顾》。内容简介：这是一部汇集了早期《 Hee Haw 》节目精华的收藏版，收录了系列开播以来的经典小品和搞笑片段。此外，还包含了超过100首脍炙人口的乡村音乐表演。更有与早期演员的独家访谈作为特别加赠内容，让观众重温那段欢乐时光。"}
{"text": "电影商品<|sid_start|><|sid_133|><|sid_276|><|sid_609|><|sid_768|><|sid_end|>代表的作品是《卡灵顿》，类型包括剧情、爱情、传记与历史等类型。剧情概要：影片改编自真实故事，讲述了英格兰画家多拉·卡灵顿与作家利顿·斯特雷奇之间一段复杂而悲伤的爱情。他们的关系充满了情感的波折与挑战，共同经历了那个时代特有的社会氛围和个人困境。。"}
{"text": "《灵异妙探：音乐剧》对应的电影商品是<|sid_start|><|sid_243|><|sid_421|><|sid_557|><|sid_768|><|sid_end|>，这是一部涵盖喜剧、悬疑与惊悚、音乐与表演等类型风格的电影。其核心剧情为：当一位疯狂的剧作家逃离精神病院，企图为他失败的戏剧演出复仇时，一场充满音乐的谋杀悬案就此展开。警探们将围绕一名过去被认为是“开膛手杰克”式戏剧的创作者展开调查，他的作品曾遭遇滑铁卢。"}
{"text": "商品<|sid_start|><|sid_234|><|sid_263|><|sid_668|><|sid_768|><|sid_end|>是一部题材和风格涵盖音乐与表演和纪录片的电影，电影名称是《Garbage：Mile High现场演唱会》。内容简介：这是一场由著名乐队Garbage于2012年在丹佛录制的完整现场演唱会。演出不仅收录了乐队最新专辑中的热门歌曲，也演唱了他们的经典代表作，为观众呈现了一场视听盛宴。此外，影片还包含精心制作的音乐录影带以及其他幕后花絮，让歌迷能够更深入地了解乐队的音乐世界。"}
```

在 **Stage 3 指令微调（SFT）阶段**，为了使模型具备**序列推荐、推荐理由生成以及基于用户实时反馈进行动态推荐调整的能力**，我们围绕“语义 ID + 用户行为序列 + 自然语言交互”设计了一组系统性的监督任务。具体而言，在训练时，该阶段通过 **序列预测、用户偏好分析、推荐理由生成、兴趣类别建模以及多轮对话推荐** 等九类任务，从单步行为建模到多轮对话决策逐级增强模型能力，使模型不仅能够预测用户下一步可能感兴趣的内容，还能够用自然语言解释推荐原因，并根据用户的正负反馈或兴趣变化动态调整推荐结果。对于验证集和测试集，只基于序列推荐任务构建，采用leave one out的方式。

在**兴趣类别建模任务**中，我们不直接使用原始商品类型，而是通过对用户历史序列进行滑动窗口统计，从商品类型中自动提取高频类别，作为用户阶段性兴趣的近似表示，从而避免人工标注并刻画兴趣的动态变化。其次，在**多轮对话推荐任务**中，我们基于真实用户行为序列构造“历史 → 推荐 → 反馈 → 再推荐”的对话结构，通过将序列中的后续行为视为用户的隐式反馈，模拟用户喜欢或不喜欢后的推荐调整过程，使模型在监督学习阶段即可接触到近似真实的交互式推荐决策。

其中部分任务举例如下：

```python
================================================================================
Task: ('category_analysis') 
[system] 你是一个专业的电影商品推荐助手。
[user] 用户最近浏览了：<|sid_start|><|sid_131|><|sid_352|><|sid_638|><|sid_768|><|sid_end|>, <|sid_start|><|sid_187|><|sid_341|><|sid_541|><|sid_768|><|sid_end|>, <|sid_start|><|sid_134|><|sid_264|><|sid_680|><|sid_768|><|sid_end|>, <|sid_start|><|sid_66|><|sid_327|><|sid_663|><|sid_768|><|sid_end|>, <|sid_start|><|sid_187|><|sid_468|><|sid_713|><|sid_768|><|sid_end|>, <|sid_start|><|sid_223|><|sid_437|><|sid_730|><|sid_768|><|sid_end|>, <|sid_start|><|sid_204|><|sid_264|><|sid_721|><|sid_768|><|sid_end|>, <|sid_start|><|sid_187|><|sid_475|><|sid_762|><|sid_768|><|sid_end|>, <|sid_start|><|sid_221|><|sid_466|><|sid_539|><|sid_768|><|sid_end|>，请分析用户偏好的内容类型。
[assistant] 用户主要感兴趣的类别是：动作与冒险、犯罪与推理、剧情。
================================================================================
Task: ('category_based_recommendation') 
[system] 你是一个专业的电影商品推荐助手。
[user] 我想要看怀旧类型的作品，请给我推荐几个。
[assistant] <|sid_start|><|sid_112|><|sid_488|><|sid_703|><|sid_768|><|sid_end|>、<|sid_start|><|sid_39|><|sid_498|><|sid_703|><|sid_768|><|sid_end|>、<|sid_start|><|sid_112|><|sid_337|><|sid_569|><|sid_768|><|sid_end|>、<|sid_start|><|sid_112|><|sid_260|><|sid_751|><|sid_769|><|sid_end|>、<|sid_start|><|sid_252|><|sid_372|><|sid_603|><|sid_768|><|sid_end|>
================================================================================
Task: ('multi_turn_dialogue_category')
[system] 你是一个专业的电影商品推荐助手。
[user] 我对悲剧类型的电影很感兴趣。
[assistant] 我推荐商品<|sid_start|><|sid_133|><|sid_365|><|sid_728|><|sid_768|><|sid_end|>, 商品<|sid_start|><|sid_133|><|sid_378|><|sid_728|><|sid_768|><|sid_end|>, 商品<|sid_start|><|sid_133|><|sid_365|><|sid_629|><|sid_768|><|sid_end|>，这些都是悲剧类型的经典电影。
[user] 换一个类型吧，我想看真人秀与生活方式。
[assistant] 好的，可以换个口味。真人秀与生活方式类型的是吧，我为你推荐商品<|sid_start|><|sid_2|><|sid_390|><|sid_635|><|sid_768|><|sid_end|>《幸存者：危地马拉》, 商品<|sid_start|><|sid_206|><|sid_316|><|sid_666|><|sid_769|><|sid_end|>《周六夜现场：第五季》, 商品<|sid_start|><|sid_2|><|sid_316|><|sid_662|><|sid_768|><|sid_end|>《蛋糕大师》，这些都是真人秀与生活方式类型的优秀作品。
================================================================================
Task: ('multi_turn_dialogue_sequence')
[system] 你是一个专业的电影商品推荐助手。
[user] 我最近看了《古代外星人：第七季》, 《远古外星人》, 商品<|sid_start|><|sid_248|><|sid_440|><|sid_609|><|sid_768|><|sid_end|>，有什么推荐的吗？
[assistant] 基于你的兴趣，我推荐商品<|sid_start|><|sid_21|><|sid_368|><|sid_766|><|sid_768|><|sid_end|>，它属于纪录片和历史类型。
[user] 我不太喜欢，换一个吧。
[assistant] 没问题，基于您的浏览历史，我推荐商品<|sid_start|><|sid_32|><|sid_320|><|sid_682|><|sid_768|><|sid_end|>《太空视角下的二战》给你，它属于纪录片和历史类型。
================================================================================
Task: ('preference_analysis') 
[system] 你是一个专业的电影商品推荐助手。
[user] 根据用户的历史浏览序列：<|sid_start|><|sid_240|><|sid_410|><|sid_710|><|sid_768|><|sid_end|>, <|sid_start|><|sid_166|><|sid_343|><|sid_540|><|sid_768|><|sid_end|>, <|sid_start|><|sid_63|><|sid_502|><|sid_649|><|sid_768|><|sid_end|>，请分析该用户的兴趣偏好。
[assistant] 该用户似乎偏爱充满动作、冒险和外星生物元素的作品。
================================================================================
Task: ('recommendation_reasoning')
[system] 你是一个专业的电影商品推荐助手。
[user] 已知用户浏览记录：<|sid_start|><|sid_240|><|sid_410|><|sid_710|><|sid_768|><|sid_end|>, <|sid_start|><|sid_166|><|sid_343|><|sid_540|><|sid_768|><|sid_end|>, <|sid_start|><|sid_63|><|sid_502|><|sid_649|><|sid_768|><|sid_end|>，请给出推荐并说明理由。
[assistant] 推荐商品：<|sid_start|><|sid_131|><|sid_421|><|sid_745|><|sid_768|><|sid_end|>

推荐理由：基于用户对类似科幻冒险题材的喜好，这部作品提供了相似的紧张刺激和探索未知宇宙的体验。
================================================================================
Task: ('semantic_anchoring')
[system] 你是一个专业的电影商品推荐助手。
[user] 已知以下电影商品：<|sid_start|><|sid_112|><|sid_267|><|sid_645|><|sid_768|><|sid_end|>，请提供其详细的信息。
[assistant] 您查询的商品是《Hee Haw：经典回顾》，这是一部喜剧、音乐与表演、其他等类型的作品。作品简介：这是一部汇集了早期《 Hee Haw 》节目精华的收藏版，收录了系列开播以来的经典小品和搞笑片段。此外，还包含了超过100首脍炙人口的乡村音乐表演。更有与早期演员的独家访谈作为特别加赠内容，让观众重温那段欢乐时光。。
================================================================================
Task: ('semantic_recovery') 
[system] 你是一个专业的电影商品推荐助手。
[user] 请根据以下商品信息返回对应的商品：
电影名：Hee Haw：经典回顾
类别：喜剧、音乐与表演、其他等
简介：这是一部汇集了早期《 Hee Haw 》节目精华的收藏版，收录了系列开播以来的经典小品和搞笑片段。此外，还包含了超过100首脍炙人口的乡村音乐表演。更有与早期演员的独家访谈作为特别加赠内容，让观众重温那段欢乐时光。
[assistant] 对应的商品为：<|sid_start|><|sid_112|><|sid_267|><|sid_645|><|sid_768|><|sid_end|>
================================================================================
Task: ('sequence_prediction')  
[system] 你是一个专业的电影商品推荐助手。
[user] 基于这些商品：<|sid_start|><|sid_131|><|sid_352|><|sid_638|><|sid_768|><|sid_end|>，预测用户下一步会点开哪一个。
[assistant] <|sid_start|><|sid_187|><|sid_341|><|sid_541|><|sid_768|><|sid_end|>
```

不同子任务的数量分布统计如下，共包含约62万条SFT数据：

![](https://clouddocs.huawei.com/koopage/v1/app/api/documents/doc/preview/db0c77cf-5019-41b2-a49d-ae3228f51601?document_id=79ae59ad-8506-4a9c-8f75-506f1bd0f245 "")

### 生成式推荐模型能力由训练任务决定

训练后的qwen3 1.7b模型具有序列预测，主题推荐，以及多轮对话的能力。而且由于训练时的多prompt设置，所以对于prompt表现出了鲁棒性，比如略微调整输入prompt内容时都能给出正确推荐格式：

```python
USER：我看了：<|sid_start|><|sid_173|><|sid_410|><|sid_722|><|sid_768|><|sid_end|>, <|sid_start|><|sid_75|><|sid_452|><|sid_736|><|sid_768|><|sid_end|>，推荐一下。
ASSISTANT: <|sid_start|><|sid_248|><|sid_338|><|sid_646|><|sid_768|><|sid_end|><|im_end|>

USER：基于这些商品：<|sid_start|><|sid_173|><|sid_410|><|sid_722|><|sid_768|><|sid_end|>, <|sid_start|><|sid_75|><|sid_452|><|sid_736|><|sid_768|><|sid_end|>，预测用户下一步会点开哪一个。
ASSISTANT:<|sid_start|><|sid_248|><|sid_338|><|sid_646|><|sid_768|><|sid_end|><|im_end|>

(为了直观显示，将模型输出的SID转化为商品标题)
USER: 基于这些商品："黑兰花"，预测用户下一步会点开哪一个，并给出推荐理由。
ASSISTANT: 推荐："最美好的时光" \n 理由：鉴于用户对探讨情感深度和人生选择的电影感兴趣，这部影片提供了一个关于时间和情感羁绊的深刻视角，能够引起用户的共鸣。

```

在主题推荐时，可以基于一些类型关键词给出推荐（为了直观，已经将模型输出的SID转化为商品标题）：

```vb
USER: 我喜欢看动物类型的电影
ASSISTANT: "传奇白马"、"狂野巴西：火与洪之歌"、"海豚的故事2"、"马歇尔的奇迹"、"地球脉动：猎与被猎"

USER: 能给我推荐一些浪漫主题的电影吗
ASSISTANT: "无法抗拒的蓝莓农场"、"尼古拉斯·斯帕克斯浪漫电影集"、"圣诞皇冠"、"红磨坊"、"罗马假日姐妹花"
```

多轮对话上，能够基于用户的喜好输出推荐内容：

```vb
USER: 我最近看了《但丁的地狱：动画史诗》, 《金刚狼与X战警：全集》, 商品"死亡空间：坠落"，有什么推荐的吗？
ASSISTANT: "死侍"
USER: 不错，再推荐一些类似的
ASSISTANT: 商品《死侍》是动作与冒险、喜剧、科幻与奇幻等类型的电影，很高兴你喜欢！那我再为你推荐一部相同类型的商品"鸦片与功夫大师"《街头霸王：暗杀之道》，它也是一部动作与冒险、剧情、犯罪与推理等类型的电影。
USER: 我不太喜欢，换一个吧
ASSISTANT: 好的，考虑到您的浏览偏好，我推荐商品"异形附身"给你，这是一部科幻与奇幻、恐怖、动作与冒险等类型的电影。
```

当前在1.7 b模型的测试上，通过设计特定的任务能够激发模型具有特定任务的功能，不过反过来模型能力也依赖任务构建的优劣，比如模型可能回复过于依赖训练数据格式导致死板。多轮对话的数据为构造对话式推荐模型提供了数据基础，当前也有一些研究基于人类问答语言学，或者通过llm构造推荐对话数据集，比如[Redial](https://arxiv.org/abs/1812.07617)，[TG\-Redial](https://arxiv.org/abs/2010.04125)，[Pearl](https://arxiv.org/abs/2403.04460)，[LLM\-Redial](https://aclanthology.org/2024.findings-acl.529/)等，这些研究中的prompt设计范式为更复杂的对话逻辑构建提供参考。

### 在序列预测任务上的准确度评估

尽管在训练时增加了多个不同的任务，但是在评估时还是只用序列预测任务进行评估。训练时采用的是leave one out的方式，比如用户历史交互序列长度为10，采用前8个用来做训练，倒数第二个和第一个分别用来做验证和测试。我们采用Hitrate和NDCG的评估指标，在评估时模型采用beam search的方式。

![](https://clouddocs.huawei.com/koopage/v1/app/api/documents/doc/preview/640d24a6-abbd-49ce-b835-a5c7488bb0fc?document_id=79ae59ad-8506-4a9c-8f75-506f1bd0f245 "")

从预测结果可以看出，序列预测任务在测试集上的效果随着模型大小增加变得更好。由于语义ID的层级性特征和llm在解码时逐个token解码，我们额外对比了前缀匹配的准确度。比如Prefix@3@10表示在beam seach为10时，预测的物品前三个SID正确的情况，可以看到非严苛全token匹配时，只匹配前缀token准确度会进一步提升，这在推荐多样性以及对待长尾或冷启物品时会有帮助。

### 多训练任务之间的能力跷跷板效应

为了让模型具有如推荐理由，基于兴趣问答能力，我们在任务构造时引入了多个任务。但在评估时由于只在序列预测任务上进行评估，我们发现额外任务的引入会对序列推荐任务准确率造成影响。

![](https://clouddocs.huawei.com/koopage/v1/app/api/documents/doc/preview/79cc7b7f-fbfb-40dd-98c8-8c53d22a7b1d?document_id=79ae59ad-8506-4a9c-8f75-506f1bd0f245 "")

在上面的图中，"1.7b"表示混合了所有任务的原始模型，"1.7b\_Seqx3"表示在原有的任务中将序列推荐任务数据扩展三倍并随机打乱，"1.7b\_onlySeq"表示只训练序列推荐任务，"1.7b\_onlySeqx3"表示只训练序列推荐任务并将数据扩展三倍。

从上图的结果能够得出一些结论：

1. 尽管序列占比任务数量在原先数据集中占比已经较大（约占比44%），进一步提升其在数据集中所占比重依旧能提升模型预测效果。

2. 如果只考虑序列预测任务，那么只训练序列任务也能起到不错的效果，事实上当前头部互联网厂商如youtube和淘宝在任务构建上确实只考虑了序列任务，不过OpenOneRec是往全能型推荐模型上走。

3. 辅助任务的构建在一定程度上促进了序列预测任务。

在单纯序列推荐上我们也将llm4Rec与模型SASRec和Bert4Rec做了对比，基于Recbole框架的默认参数，并没有对序列模型做精细的超参优化，结果表明SASRec和Bert4Rec在Hit@1上的准确度都远低于llm4rec，分别为0.0049和0.012，但是在Hit@10上，SASRec模型能达到0.10，Bert4Rec为0.05。但这个结果并不是恒定的，比如这是amazon review的电影数据集的结果，在amazon review的乐器数据集中，llm4rec平均优于SASRec和Bert4Rec约20%。

考虑到对电影推荐类，只用历史交互序列最后一个商品作为golden truth会受到序列质量影响，这里我们额外使用了外部的裁判大模型基于llm4rec与序列模型的推荐结果进行打分，prompt格式为：

```python
你是一个专业电影推荐系统评估专家。
下面是一个用户的历史观看序列（按时间顺序）：
{history}
下面是三个不同模型给出的“下一部电影”推荐：
模型A：
{text1}
模型B：
{text2}
模型C：
{text3}
请你站在“电影推荐系统”的角度，从以下方面综合打分（1–5分，5分最好）：
- 是否符合用户历史的题材与风格
- 是否在语义上与历史序列连贯
- 是否是一个合理且有吸引力的推荐
请给出 JSON 格式的输出，例如：
{{
  "模型A": 5,
  "模型B": 4,
  "模型C": 3
}}
```

随机采样了测试集中的N=200条数据做评估，下图显示了裁判模型的评分区间，表明llm4rec的推荐效果优于序列推荐模型：

![](https://clouddocs.huawei.com/koopage/v1/app/api/documents/doc/preview/1ba92b13-c20a-42fa-b0ac-48785738483f?document_id=79ae59ad-8506-4a9c-8f75-506f1bd0f245 "")

### 基于tolen粒度的loss与基于item粒度的loss

在sft后尽管loss已经降低至0.8，但我们发现模型对于序列预测任务在训练集上的表现并不好。下图对比了几种不同的训练策略训练后的模型在序列预测任务中训练集上的准确度（采用非beam search方式，"temperature": 0.6, "top\_k": 20,"top\_p": 0.95）：

![](https://clouddocs.huawei.com/koopage/v1/app/api/documents/doc/preview/8dc43456-3ea7-43b0-81b9-212ee52f2956?document_id=79ae59ad-8506-4a9c-8f75-506f1bd0f245 "")

结果可以看出，在1.7b模型训练3 epoch下，模型正常推理时在序列预测上训练集的准确率很低，尽管全局loss已经下降至0.75。这主要是由于每一个商品都是由四层SID组成，在训练时通过tearcher\-forcing机制训练token\-level loss，而在推理时需要exact match；此外当多个任务同时存在于训练集时，好学习的任务loss会影响难学习的任务。当模型大小升为8b后，对SID的学习能力显著增强，不需要对序列数据进行扩增也能学习到高的准确值。

下面是对loss的一个理论分析：

以四层sid为例，设 ground truth 序列为（忽略 start/end 的固定符号，只看 4 个 sid token）：

$$ y=(y_1, y_2, y_3, y_4) $$

其中每个$y_t$是某个 sid token，例如：

$$ y = (58, 384, 539, 768) $$

SFT 训练时的 teacher\-forcing loss（只在 assistant token 上计算）为：

$$ \mathcal L=-\frac{1}{4}\sum_{t=1}^4logp_{\theta}(y_t \mid x,y_{<t}) $$

其中$x$表示 system+user 的条件输入，$y_{<t}$是真实的前缀（teacher forcing）。

推理时通常用 greedy / beam 得到一个序列$\hat{y}$，Hit@1 定义为：

$$ \mathrm{Hit@1} = 1[\hat{y}=y] $$

这是一个序列级别的 0/1 指标。 

为了把“loss 的量纲”变成直觉，我们用一个常见近似：如果每个位置上模型对正确 token 的平均概率相近，即

$$ p_{\theta}(y_t\mid x,y_{<t})\approx p $$

那么 teacher\-forcing 的平均 loss 近似为： 

$$ \mathcal{L} \approx-logp \Rightarrow p \approx e^{-\mathcal{L}_{TF}} $$

若训练集 $loss \approx 0.8$，则

$$ p \approx e^{-0.8} \approx 0.449 $$

在一个“理想化”场景里（每一步独立、且 greedy 每步正确概率约等于 ），完整 4 步全对概率约为：

$$ p(\hat{y} = y) \approx p^4 \approx 0.449^4 \approx 0.0407 $$

即约 4.1%。loss=0.8 在 token\-level 看很不错，但 exact\-match 在 4 步序列上天然会被“乘法效应”压得很低。

同理，如果$\mathcal{L} = 1.0$，则$p \approx 0.367$，完整匹配约：

$$ 0.367^4 \approx 0.018 $$

只剩 1.8%。

上面的$p^4$只是“乘法效应”，但在实际生成中还有一个更关键的差异：

* 训练（teacher forcing）：第 t 步条件是真实前缀 $y_{<t}$

* 推理（free\-running）：第 t 步条件是模型前缀 $\hat y_{<t}$

训练优化的是：

$$ p_{\theta}(y_t\mid x,y_{<t}) $$

但推理过程中真正需要的是：

$$ p_{\theta}(y_t\mid x, \hat y_{<t}) $$

一旦前面一步错了$(\hat y_{t-1} \not = y_{t-1})$，后续分布可能显著漂移，导致错误连锁放大。导致 exposure bias。

从上面的结果有如下三个观点，对于如1.7b的小模型，尽可能只设计序列预测任务，如果需要多个任务叠加，需要增加序列预测任务的权重或者增加训练轮数使得模型能充分学习到SID的序列信息；对于8b的大模型，可以在基于序列任务的基础上进行额外任务的叠加。

### 冷启动的评测

生成式推荐在推理时模型会逐token生成SID，所以可能会生成训练数据集中不存在SID（empty），或者生成在序列中出现次数很少的商品（交互次数&lt;=2）。我们对模型beam search=10推理下生成的SID进行归类，结果如下：

![](https://clouddocs.huawei.com/koopage/v1/app/api/documents/doc/preview/d967547c-d762-44b6-b44b-10b7e0ec279d?document_id=79ae59ad-8506-4a9c-8f75-506f1bd0f245 "")

结果表明当beam search为1时，输出的不在商品列表里的SID序列概率为0，随着beam search索引增大，模型更有可能推出长尾或者不在商品列表里的SID。在beam search=10时，分别达到千分之8和千分之2。可以看出，llm4rec对冷启动商品友好。为了避免生成无效的SID，可以基于前缀约束解码的方式， 在生成的语义ID中，有效语义ID占比超过98%（beam search=10），考虑到受限路径会增加推理耗时，实验中可以把受限路径给去掉。

### 思考与优化方向

在做生成式推荐落地时，需要思考是把模型当做什么形式的存在，是推荐助手还是单纯的序列推荐模型用来做生成式召回。当前淘宝的发布的生成式召回模型ALGR以及谷歌PLUM模型都是将LLM用于纯序列推荐任务，那么构造prompt时就无需考虑保留原LLM的能力，prompt设计需要融入用户画像，点击序列等且尽可能以简洁高效为主。对于推荐助手，尽管我们的训练发现在训练集特定任务的引入能够赋予LLM特定能力，但是能力跷跷板效应可能会影响序列推荐的效果，此外构造任务时prompt的设计不完善可能会引入人为偏见。

当前生成式推荐的部署往往需要在已有推荐系统上局部优化，需要实时训练的能力。因此相比于如何用更高超的训练技巧得到更好的离线指标，尽快部署上线随着线上模型迭代才是并基于A/B test实验才是验证效果的最好方式。考虑到大模型推理时延，一般会采用异步+缓存的方式，通过如何通过近线引擎实时处理埋点数据，如何进行样本采样与在线模型跌打与发布这样的工程化的能力需要进一步关注。

在序列推荐任务训练中，模型基于next token loss的训练方式不太适用于让模型批量推理出多个的场景，尽管可以通过beam search，但会造成同类型商品堆叠，失去多样性。因此在训练方式上需要从单一商品预测到多商品预测的改进。

beam search推理十分耗时，并且当需要某些商品满足特定约束是，需要前缀约束+beam search，需要推理性能优化。
